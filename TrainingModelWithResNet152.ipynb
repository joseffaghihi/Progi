{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TrainingModelWithResNet152.ipynb","provenance":[{"file_id":"1nO5qudJ9O7xUTxB8ilk5JxnZP83dNSij","timestamp":1635860878922},{"file_id":"1vrDHMQRwzsjq2z8vT5IaVfZAA0Txa8hm","timestamp":1627893901627},{"file_id":"1Q4Ia0pB1nnJIrU6_BVlUKvORKYAmf_8M","timestamp":1627812191892}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d10680527d354cd4859802247e68e55f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_942bfed6e25b48b4836620e2c798ab35","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0885c512f7114710a54c2ebd73138329","IPY_MODEL_57260f3c3cce46c2a0569f354d78772b","IPY_MODEL_d9719147adf8494fabc20e04ced24433"]}},"942bfed6e25b48b4836620e2c798ab35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0885c512f7114710a54c2ebd73138329":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b67df3d57c494b4abaab600190dcc734","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eaff0f925a174a8987d4a0d0f7b86810"}},"57260f3c3cce46c2a0569f354d78772b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1b1f216f25f44bf5ac74e8629ad46840","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":241627721,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":241627721,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac7f631a0cb140c6a8c02144f225a049"}},"d9719147adf8494fabc20e04ced24433":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2e33d7ca5372487ba2bdc2884d3126c2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 230M/230M [00:01&lt;00:00, 140MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0bc6142269d944b79b62d60f39460130"}},"b67df3d57c494b4abaab600190dcc734":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eaff0f925a174a8987d4a0d0f7b86810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b1f216f25f44bf5ac74e8629ad46840":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ac7f631a0cb140c6a8c02144f225a049":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e33d7ca5372487ba2bdc2884d3126c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0bc6142269d944b79b62d60f39460130":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"5PBShNP86rti","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637446399494,"user_tz":300,"elapsed":9076,"user":{"displayName":"Makhtar Ndiaye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc9FIEKGjHQ8jmIzyaiDHQVUjq977mUnl9uTObWg=s64","userId":"04628510915637787867"}},"outputId":"41dd62e5-8773-489e-bc1b-242c15036aa9"},"source":["# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","    process = psutil.Process(os.getpid())\n","    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n","    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gputil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","Building wheels for collected packages: gputil\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=a68067c6d013030f34e2d6a29f5bec1e9faa9b5ef93fb3875a7d15efb34e89b7\n","  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n","Successfully built gputil\n","Installing collected packages: gputil\n","Successfully installed gputil-1.4.0\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n","Gen RAM Free: 26.0 GB  |     Proc size: 1.2 GB\n","GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total     16280MB\n"]}]},{"cell_type":"code","metadata":{"id":"XlXkQ6qx_Xay"},"source":["# Imports here\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","import torchvision.models as models\n","from PIL import Image\n","import json\n","from matplotlib.ticker import FormatStrFormatter\n","import os\n","import shutil\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYdee71UJDSF"},"source":["# **Steps** <p>\n","Step 1: Load Dataset <p>\n","Step 2: Transform the Dataset <p>\n","Step 3: Create Model <p>\n","Step 4: Train Model <p>\n","Step 5: Save the Model <p>\n","<!-- Step 6: Load the Model <p>\n","Step 7: Predict the Image <p>\n","Step 8: Show the result -->"]},{"cell_type":"markdown","metadata":{"id":"4T7noSJhJUFr"},"source":["## Step 1: Load Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5pJwKuetJ6O","executionInfo":{"status":"ok","timestamp":1637492156343,"user_tz":300,"elapsed":21348,"user":{"displayName":"Makhtar Ndiaye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc9FIEKGjHQ8jmIzyaiDHQVUjq977mUnl9uTObWg=s64","userId":"04628510915637787867"}},"outputId":"e25bb848-9e23-4bb7-c075-0e0e4109634a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"74CbnuuJAx4Y"},"source":["data_dir = \"/content/drive/MyDrive/car_data\"\n","train_dir = data_dir + '/train'\n","valid_dir = data_dir + '/valid'\n","test_dir = data_dir + '/test'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LLysmbnmJemX"},"source":["## Step 2: Transform the Dataset"]},{"cell_type":"code","metadata":{"id":"zOqT-9GXA8F_"},"source":["# Training transform includes random rotation and flip to build a more robust model\n","train_transforms = transforms.Compose([transforms.Resize((224,224)),\n","                                       transforms.RandomRotation(30),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","# The validation set will use the same transform as the test set\n","test_transforms = transforms.Compose([transforms.Resize((224,224)),\n","                                      transforms.CenterCrop(200),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","validation_transforms = transforms.Compose([transforms.Resize((224,224)),\n","                                            transforms.CenterCrop(200),\n","                                            transforms.ToTensor(),\n","                                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","# Load the datasets with ImageFolder\n","train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n","test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n","valid_data = datasets.ImageFolder(data_dir + '/valid', transform=validation_transforms)\n","\n","# Using the image datasets and the trainforms, define the dataloaders\n","# The trainloader will have shuffle=True so that the order of the images do not affect the model\n","trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n","testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n","validloader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M265WLOdJ3xV"},"source":["## Step 3: Create Model"]},{"cell_type":"code","metadata":{"id":"1emIi-zkXNz1","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d10680527d354cd4859802247e68e55f","942bfed6e25b48b4836620e2c798ab35","0885c512f7114710a54c2ebd73138329","57260f3c3cce46c2a0569f354d78772b","d9719147adf8494fabc20e04ced24433","b67df3d57c494b4abaab600190dcc734","eaff0f925a174a8987d4a0d0f7b86810","1b1f216f25f44bf5ac74e8629ad46840","ac7f631a0cb140c6a8c02144f225a049","2e33d7ca5372487ba2bdc2884d3126c2","0bc6142269d944b79b62d60f39460130"]},"executionInfo":{"status":"ok","timestamp":1637492324534,"user_tz":300,"elapsed":3735,"user":{"displayName":"Makhtar Ndiaye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc9FIEKGjHQ8jmIzyaiDHQVUjq977mUnl9uTObWg=s64","userId":"04628510915637787867"}},"outputId":"9a73ac64-fb98-4f68-8fc8-81488fb0635f"},"source":["model = models.resnet152(pretrained=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d10680527d354cd4859802247e68e55f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/230M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"ZcqibDULVbil"},"source":["num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, 199)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZQGN3U1tWu0"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold = 0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1j802WQKhRa"},"source":["## Step 4: Train Model"]},{"cell_type":"code","metadata":{"id":"Z7OXITrCGu5J"},"source":["# Implement a function for the validation pass\n","def validation(model, validloader, criterion):\n","    valid_loss = 0\n","    accuracy = 0\n","    \n","    # change model to work with cuda\n","    model.to('cuda')\n","\n","    # Iterate over data from validloader\n","    for ii, (images, labels) in enumerate(validloader):\n","    \n","        # Change images and labels to work with cuda\n","        images, labels = images.to('cuda'), labels.to('cuda')\n","\n","        # Forward pass image though model for prediction\n","        output = model.forward(images)\n","        # Calculate loss\n","        valid_loss += criterion(output, labels).item()\n","        # Calculate probability\n","        ps = torch.exp(output)\n","        \n","        # Calculate accuracy\n","        equality = (labels.data == ps.max(dim=1)[1])\n","        accuracy += equality.type(torch.FloatTensor).mean()\n","    \n","    return valid_loss, accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBpYU-KpuyFq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637501538646,"user_tz":300,"elapsed":9197222,"user":{"displayName":"Makhtar Ndiaye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc9FIEKGjHQ8jmIzyaiDHQVUjq977mUnl9uTObWg=s64","userId":"04628510915637787867"}},"outputId":"67da9925-9692-40f2-f47b-3d220c651821"},"source":["epochs = 10\n","steps = 0\n","print_every = 30\n","\n","# change to gpu mode\n","model.to('cuda')\n","model.train()\n","for e in range(epochs):\n","\n","    running_loss = 0\n","    \n","    # Iterating over data to carry out training step\n","    for ii, (inputs, labels) in enumerate(trainloader):\n","\n","        steps += 1\n","        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n","        # zeroing parameter gradients\n","\n","        optimizer.zero_grad()\n","        # Forward and backward passes\n","\n","        outputs = model.forward(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item()\n","        \n","        # Carrying out validation step\n","        if steps % print_every == 0:\n","            # setting model to evaluation mode during validation\n","            model.eval()\n","            \n","            # Gradients are turned off as no longer in training\n","            with torch.no_grad():\n","                valid_loss, accuracy = validation(model, validloader, criterion)\n","            \n","            print(f\"No. epochs: {e+1}, \\\n","            Training Loss: {round(running_loss/print_every,3)} \\\n","            Valid Loss: {round(valid_loss/len(validloader),3)} \\\n","            Valid Accuracy: {round(float(accuracy/len(validloader)),3)}\")\n","            \n","            # Turning training back on\n","            model.train()\n","            lrscheduler.step(accuracy * 100)\n","           "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No. epochs: 1,             Training Loss: 5.268             Valid Loss: 5.152             Valid Accuracy: 0.017\n","No. epochs: 1,             Training Loss: 9.957             Valid Loss: 4.295             Valid Accuracy: 0.092\n","No. epochs: 1,             Training Loss: 13.894             Valid Loss: 3.558             Valid Accuracy: 0.181\n","No. epochs: 1,             Training Loss: 17.192             Valid Loss: 3.064             Valid Accuracy: 0.241\n","No. epochs: 2,             Training Loss: 1.228             Valid Loss: 2.738             Valid Accuracy: 0.322\n","No. epochs: 2,             Training Loss: 3.393             Valid Loss: 2.321             Valid Accuracy: 0.387\n","No. epochs: 2,             Training Loss: 5.342             Valid Loss: 2.134             Valid Accuracy: 0.414\n","No. epochs: 2,             Training Loss: 7.051             Valid Loss: 1.808             Valid Accuracy: 0.506\n","No. epochs: 2,             Training Loss: 8.623             Valid Loss: 1.896             Valid Accuracy: 0.485\n","No. epochs: 3,             Training Loss: 1.13             Valid Loss: 1.408             Valid Accuracy: 0.621\n","No. epochs: 3,             Training Loss: 2.066             Valid Loss: 1.022             Valid Accuracy: 0.747\n","No. epochs: 3,             Training Loss: 2.875             Valid Loss: 0.939             Valid Accuracy: 0.777\n","No. epochs: 3,             Training Loss: 3.63             Valid Loss: 0.899             Valid Accuracy: 0.785\n","No. epochs: 4,             Training Loss: 0.298             Valid Loss: 0.871             Valid Accuracy: 0.789\n","No. epochs: 4,             Training Loss: 0.897             Valid Loss: 0.852             Valid Accuracy: 0.79\n","No. epochs: 4,             Training Loss: 1.523             Valid Loss: 0.85             Valid Accuracy: 0.792\n","No. epochs: 4,             Training Loss: 2.143             Valid Loss: 0.84             Valid Accuracy: 0.794\n","No. epochs: 4,             Training Loss: 2.743             Valid Loss: 0.839             Valid Accuracy: 0.797\n","No. epochs: 5,             Training Loss: 0.563             Valid Loss: 0.833             Valid Accuracy: 0.797\n","No. epochs: 5,             Training Loss: 1.137             Valid Loss: 0.835             Valid Accuracy: 0.8\n","No. epochs: 5,             Training Loss: 1.703             Valid Loss: 0.838             Valid Accuracy: 0.798\n","No. epochs: 5,             Training Loss: 2.306             Valid Loss: 0.836             Valid Accuracy: 0.799\n","No. epochs: 6,             Training Loss: 0.29             Valid Loss: 0.836             Valid Accuracy: 0.8\n","No. epochs: 6,             Training Loss: 0.848             Valid Loss: 0.835             Valid Accuracy: 0.799\n","No. epochs: 6,             Training Loss: 1.444             Valid Loss: 0.835             Valid Accuracy: 0.797\n","No. epochs: 6,             Training Loss: 2.019             Valid Loss: 0.838             Valid Accuracy: 0.798\n","No. epochs: 6,             Training Loss: 2.595             Valid Loss: 0.836             Valid Accuracy: 0.799\n","No. epochs: 7,             Training Loss: 0.579             Valid Loss: 0.838             Valid Accuracy: 0.8\n","No. epochs: 7,             Training Loss: 1.168             Valid Loss: 0.836             Valid Accuracy: 0.797\n","No. epochs: 7,             Training Loss: 1.758             Valid Loss: 0.835             Valid Accuracy: 0.798\n","No. epochs: 7,             Training Loss: 2.324             Valid Loss: 0.842             Valid Accuracy: 0.799\n","No. epochs: 8,             Training Loss: 0.289             Valid Loss: 0.835             Valid Accuracy: 0.796\n","No. epochs: 8,             Training Loss: 0.855             Valid Loss: 0.838             Valid Accuracy: 0.799\n","No. epochs: 8,             Training Loss: 1.4             Valid Loss: 0.839             Valid Accuracy: 0.8\n","No. epochs: 8,             Training Loss: 1.989             Valid Loss: 0.837             Valid Accuracy: 0.797\n","No. epochs: 8,             Training Loss: 2.585             Valid Loss: 0.835             Valid Accuracy: 0.8\n","No. epochs: 9,             Training Loss: 0.553             Valid Loss: 0.835             Valid Accuracy: 0.797\n","No. epochs: 9,             Training Loss: 1.116             Valid Loss: 0.839             Valid Accuracy: 0.799\n","No. epochs: 9,             Training Loss: 1.696             Valid Loss: 0.834             Valid Accuracy: 0.798\n","No. epochs: 9,             Training Loss: 2.276             Valid Loss: 0.836             Valid Accuracy: 0.796\n","No. epochs: 10,             Training Loss: 0.295             Valid Loss: 0.838             Valid Accuracy: 0.795\n","No. epochs: 10,             Training Loss: 0.851             Valid Loss: 0.838             Valid Accuracy: 0.798\n","No. epochs: 10,             Training Loss: 1.45             Valid Loss: 0.837             Valid Accuracy: 0.799\n","No. epochs: 10,             Training Loss: 2.018             Valid Loss: 0.833             Valid Accuracy: 0.8\n","No. epochs: 10,             Training Loss: 2.58             Valid Loss: 0.834             Valid Accuracy: 0.798\n"]}]},{"cell_type":"code","metadata":{"id":"cXyFWRKJIaqU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637502043745,"user_tz":300,"elapsed":425265,"user":{"displayName":"Makhtar Ndiaye","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjc9FIEKGjHQ8jmIzyaiDHQVUjq977mUnl9uTObWg=s64","userId":"04628510915637787867"}},"outputId":"baf2a494-eb65-4231-8c25-48a886cf31c9"},"source":["correct = 0\n","total = 0\n","model.to('cuda')\n","\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        images, labels = images.to('cuda'), labels.to('cuda')\n","\n","        # Get probabilities\n","        outputs = model(images)\n","        # Turn probabilities into predictions\n","        _, predicted_outcome = torch.max(outputs.data, 1)\n","        # Total number of images\n","        total += labels.size(0)\n","        # Count number of cases in which predictions are correct\n","        correct += (predicted_outcome == labels).sum().item()\n","\n","print(f\"Test accuracy of model: {round(100 * correct / total,3)}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy of model: 76.524%\n"]}]},{"cell_type":"markdown","metadata":{"id":"dekV68M3Kufx"},"source":["## Step 5: Save the Model"]},{"cell_type":"code","metadata":{"id":"5vcTL2RZBdDk"},"source":["# Saving: feature weights, new model.fc, index-to-class mapping, optimiser state, and No. of epochs\n","checkpoint = {'state_dict': model.state_dict(),\n","              'model': model.fc,\n","              'class_to_idx': train_data.class_to_idx,\n","              'opt_state': optimizer.state_dict,\n","              'num_epochs': epochs}\n","\n","torch.save(checkpoint, '/content/drive/MyDrive/resnet152_weights_saved_new.pth')\n","#model.save('resnet101_on_vehicledetection')"],"execution_count":null,"outputs":[]}]}